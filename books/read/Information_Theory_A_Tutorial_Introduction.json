{
    "meta": {
        "title": "Information Theory: A Tutorial Introduction",
        "author": "James V Stone",
        "category": "Educational",
        "publisher": "Sebtel Press",
        "pages": 257
    },
    "parts": [
        {
            "title": "Introduction to Information Theory",
            "body": "<b style='color: forestgreen;'>Information Theory</b> explores how <b style='color: forestgreen;'>information</b> plays a critical role in various domains such as <b style='color: forestgreen;'>computers</b>, <b style='color: forestgreen;'>evolution</b>, <b style='color: forestgreen;'>physics</b>, and more. At its core is Claude <b style='color: forestgreen;'>Shannon</b>'s influential work, which transformed our understanding of <b style='color: forestgreen;'>information</b> from a vague concept to a precise, measurable quantity. The ability to distinguish between <b style='color: forestgreen;'>signal</b> (useful information) and <b style='color: forestgreen;'>noise</b> (irrelevant data) is vital both in technology and nature, assisting in tasks like compressing TV signals and understanding how <b style='color: forestgreen;'>brains</b> process sensory information. One <b style='color: forestgreen;'>bit</b> of information is the amount needed to choose between two equally probable options, akin to making decisions at a fork in the road. Information theory fundamentally helps in compressing data and understanding natural phenomena, suggesting deep connections between <b style='color: forestgreen;'>information</b>, <b style='color: forestgreen;'>communication</b>, and even <b style='color: forestgreen;'>evolution</b>."
        },
        {
            "title": "Information Theory Basics with Coin Flip Example",
            "body": "This section explains <b style='color: forestgreen;'>information</b> theory concepts using a <b style='color: forestgreen;'>coin flip</b> and other examples. A random <b style='color: forestgreen;'>variable</b> (like X) maps outcomes of an <b style='color: forestgreen;'>experiment</b> (e.g., flipping a coin) to numerical values, which are <b style='color: forestgreen;'>discrete</b> in nature. A <b style='color: forestgreen;'>fair coin</b> will have equal probabilities for heads and tails (50/50), while an <b style='color: forestgreen;'>unfair coin</b> might favor heads (e.g., 90%). The text emphasizes the difference between outcomes and the numerical outcome values, using coin flips as an example.\n\nWe learn about a <b style='color: forestgreen;'>communication channel</b> where a message is encoded into codewords and sent through a channel that may have noise causing errors. <b style='color: forestgreen;'>Channel capacity</b> is the maximum rate of information a channel can handle, and it's affected by noise. A noiseless channel\u2019s capacity equals its data rate, transmitting information efficiently. In contrast, a <b style='color: forestgreen;'>noisy channel</b> has reduced capacity, affected by the introduced noise. The goal in these scenarios is to minimize <b style='color: forestgreen;'>errors</b> and match data rates with channel capacity to optimize communication."
        },
        {
            "title": "Information and Entropy Concepts",
            "body": "<h3 style='color: forestgreen;'>Summary of Information, Surprise, and Entropy</h3>\n\nIn <b style='color: forestgreen;'>information theory</b>, entropy represents the average amount of <b style='color: forestgreen;'>surprise</b> or <b style='color: forestgreen;'>Shannon information</b> associated with a random variable based on the probability distribution of its outcomes. Shannon proposes that the less likely an outcome, the more surprising and informative it is. The entropy, denoted as <b style='color: forestgreen;'>H(X)</b>, indicates the average surprise for a given set of possible values based on their probability distribution. This can be thought of as measuring the uncertainty about the variable's value.\n\nFor a <b style='color: forestgreen;'>fair coin</b>, where heads and tails are equally probable, the entropy is at its maximum because there is no predictability (each outcome is 1 bit of information). However, for a <b style='color: forestgreen;'>biased coin</b> (e.g., 90% chance heads), entropy decreases because the outcome is more predictable, thus requiring fewer bits to describe the average surprise. The average surprise (entropy) of such biased distributions can be calculated using the logarithmic function, which reflects the extent of information or unpredictability in the outcomes."
        },
        {
            "title": "Entropy Estimation & Letter Correlation",
            "body": "In analyzing the <b style='color: forestgreen;'>entropy of English letters</b>, it's pivotal to understand how dependency plays a role. <b style='color: forestgreen;'>Shannon's theorem</b> helps decompose text data into blocks, making it possible to determine how much <b style='color: forestgreen;'>information</b> or <b style='color: forestgreen;'>uncertainty</b> is in a letter. As block length (number of letters) increases, the ability to predict letters within that block typically increases, thus <b style='color: forestgreen;'>decreasing uncertainty</b>. For example, blocks of <b style='color: forestgreen;'>3 letters have less entropy than those of 2</b>. This process allows us to calculate a more accurate estimate of the <b style='color: forestgreen;'>entropy of English</b> compared to estimates based solely on single letter frequencies which don't account for such dependencies. <b style='color: forestgreen;'>Shannon's coding theorem suggests</b> that for English text, long enough blocks make most of the sequences <b style='color: forestgreen;'>independent</b>, meaning efficient coding is possible using these principles.\n\nAs block lengths increase past a <b style='color: forestgreen;'>correlation length</b>, the methods to estimate entropy stabilize, meaning improvements in predicting or reducing uncertainty <b style='color: forestgreen;'>plateau</b>. Recognizing dependencies over these longer lengths helps reduce surprise about what the next letter could be. The entropy of English text is therefore <b style='color: forestgreen;'>lower than one might initially estimate</b>, thanks to these long-range <b style='color: forestgreen;'>probabilistic relationships</b>. This principle also applies broadly beyond language, such as images and DNA, where sequences aren't completely independent at short lengths but tend towards independence over longer blocks."
        },
        {
            "title": "Kolmogorov Complexity and Shannon's Theorem",
            "body": "In this section, the <b style='color: forestgreen;'>Kolmogorov complexity</b> of the number <b style='color: forestgreen;'>\u03c0</b> is discussed. It's the length of the <b style='color: forestgreen;'>shortest program</b> needed to generate \u03c0, which contains many fewer bits than its digits. The Kolmogorov complexity concept connects to Shannon's <b style='color: forestgreen;'>information theory</b>, where the complexity of a sequence <b style='color: forestgreen;'>approximates its entropy</b>. However, determining Kolmogorov complexity is <b style='color: forestgreen;'>non-computable</b>, meaning there's no algorithm to definitively find it, except by trying all possible <b style='color: forestgreen;'>computer programs</b>. Even with a concise program for \u03c0, it's unsure if a shorter one exists.\n\nThe chapter progresses to <b style='color: forestgreen;'>Shannon's source coding theorem</b>, which deals with efficiently encoding messages, related to <b style='color: forestgreen;'>data compression</b>. The concept of <b style='color: forestgreen;'>channel capacity</b>\u2014the max rate to send information\u2014is essential here. Shannon's theorem shows some codes aren't efficient, meaning each bit may hold less than the maximum possible information. Yet, by encoding, dependent symbols can become independent, making Shannon's theorem applicable to <b style='color: forestgreen;'>natural sequences</b> like language or images."
        },
        {
            "title": "Error Correcting Codes and Block Codes",
            "body": "When transmitting information over a <b style='color: forestgreen;'>noisy channel</b>, adding redundancy to the message using <b style='color: forestgreen;'>error correcting codes</b> is helpful. One type, <b style='color: forestgreen;'>block codes</b>, arranges a message into a grid and adds parity bits to detect and correct errors. A <b style='color: forestgreen;'>parity bit</b> added to each row and column helps identify errors. For example, if a message is changed in transmission, the parity bits make it easy to detect and correct the error. However, this increases the total transmitted data.\n\nIncreasing the number of binary digits with parity bits is called <b style='color: forestgreen;'>parity overhead</b>. For a <b style='color: forestgreen;'>4x4 block</b>, one error can be corrected per <b style='color: forestgreen;'>24 digits</b>. Making the block larger, like a <b style='color: forestgreen;'>20x20 block</b>, corrects one error out of <b style='color: forestgreen;'>440 digits</b>, but with less overhead. Balancing <b style='color: forestgreen;'>robustness</b> and additional data transmitted is essential. Redundancy aids error detection, but too much decreases efficiency in noiseless channels. Redundancy makes messages less susceptible to noise, while reducing it makes them vulnerable."
        },
        {
            "title": "Mutual Information and Channels",
            "body": "<b style='color: forestgreen;'>Communication</b> involves reproducing messages at one point, either exactly or approximately, from another point. <b style='color: forestgreen;'>Mutual Information</b> for continuous variables tells us how many input values can be reliably distinguished by knowing the output values. It's symmetric, meaning you get the same amount from output values about input values too. Equations for mutual information tell us about the average information conveyed. <b style='color: forestgreen;'>Mutual Information</b> is unaffected by how complex the relationship between two variables is. It's measured as bits, and applies no matter how you transform those variables, as long as the transformation is reversible."
        },
        {
            "title": "Channel Capacity and Gaussian Distribution",
            "body": "In Chapter 7, the discussion focuses on the <b style='color: forestgreen;'>Gaussian Channel</b>, which deals with maximizing the <b style='color: forestgreen;'>channel capacity</b> or the amount of information that can be transmitted. The <b style='color: forestgreen;'>channel capacity</b> is computed using the formula <b style='color: forestgreen;'>C = max H(Y) - H(\u03b7)</b>, where <b style='color: forestgreen;'>\u03b7</b> represents the noise. To maximize <b style='color: forestgreen;'>mutual information</b>, we can\u2019t reduce the noise, but can maximize the entropy of <b style='color: forestgreen;'>Y</b>, the output signal. A <b style='color: forestgreen;'>Gaussian</b> distribution is ideal for maximizing this entropy when variance is fixed. Therefore, if both input, output, and noise are Gaussian, the channel reaches its capacity. For channel variance, <b style='color: forestgreen;'>P/N</b>, where <b style='color: forestgreen;'>P</b> is signal power and <b style='color: forestgreen;'>N</b> is noise power, the channel capacity is defined by <b style='color: forestgreen;'>Shannon's equation</b>: <b style='color: forestgreen;'>C = \u00bd log(1 + P/N)</b>, which describes the relationship between a signal\u2019s variance and the noise's variance, essentially the signal to noise ratio (SNR)."
        },
        {
            "title": "Maxwell's Demon: Information and Thermodynamics",
            "body": "<b style='color: forestgreen;'>Maxwell's demon</b> is a thought experiment that illustrates the <b style='color: forestgreen;'>relationship</b> between <b style='color: forestgreen;'>information</b> and <b style='color: forestgreen;'>thermodynamic entropy.</b> The demon can sort <b style='color: forestgreen;'>fast</b> and <b style='color: forestgreen;'>slow molecules</b> in a gas, creating a <b style='color: forestgreen;'>temperature difference</b> useful for doing work, like running a power station. While this seems to defy physics by generating <b style='color: forestgreen;'>free energy</b>, the act of gathering information expends energy equal to the energy gained, maintaining balance and confirming that <b style='color: forestgreen;'>energy can't be created from information</b> alone.\n\nShannon\u2019s information entropy connects with thermodynamic entropy, emphasizing that <b style='color: forestgreen;'>no system</b> can achieve endless energy production without exerting an equivalent energy cost. This underscores a fundamental <b style='color: forestgreen;'>unification</b> of information and physical laws, reaffirming that <b style='color: forestgreen;'>physical theories</b> must acknowledge both thermodynamic and information-theoretic principles, as reflected in Terry Pratchett's metaphor about cosmic information storage."
        },
        {
            "title": "Information Theory Overview",
            "body": "<b style='color: forestgreen;'>Information Theory</b> was developed by Claude <b style='color: forestgreen;'>Shannon</b> in the 1940s. It's crucial now in fields like <b style='color: forestgreen;'>telecommunications</b>, <b style='color: forestgreen;'>genetics</b>, and <b style='color: forestgreen;'>brain sciences</b>. This book explains these theories with easy examples like the game \"<b style='color: forestgreen;'>20 questions</b>.\" More advanced topics cover connections between <b style='color: forestgreen;'>information theory</b> and <b style='color: forestgreen;'>thermodynamic entropy</b> and its uses in <b style='color: forestgreen;'>biology</b> and <b style='color: forestgreen;'>communication</b>."
        }
    ]
}