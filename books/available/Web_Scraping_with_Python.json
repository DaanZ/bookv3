{
    "meta": {
        "title": "Web Scraping with Python",
        "author": "Ryan Mitchell",
        "category": "Programming",
        "publisher": "O'Reilly Media, Inc.",
        "pages": 306
    },
    "parts": [
        {
            "title": "Summary of Preface and Context",
            "body": "The book <b style='color: forestgreen;'>Web Scraping with Python</b> by <b style='color: forestgreen;'>Ryan Mitchell</b> is a comprehensive guide to collecting and using data from the web. It addresses common questions and misconceptions about web scraping, emphasizing that it is legal, and tackles challenges like JavaScript-heavy pages and login requirements. Web scraping, often called <b style='color: forestgreen;'>web crawling</b> or using <b style='color: forestgreen;'>bots</b>, allows automated data gathering from the internet, typically bypassing traditional <b style='color: forestgreen;'>APIs</b> when they don't suffice. This book is not for Python beginners but aims to cover skills ranging from <b style='color: forestgreen;'>basic web scraping techniques</b> to advanced topics, structured in two main parts. The <b style='color: forestgreen;'>first part</b> is essential for understanding core mechanics, including parsing HTML and storing data, while the <b style='color: forestgreen;'>second part</b> delves into advanced applications, stressing web scraping's significant productivity and informational benefits across fields like market analysis and even art."
        },
        {
            "title": "Understanding Web Scraping Basics and Initial Setup",
            "body": "<h3 style='color: forestgreen;'>Breaking Down Web Scraping Basics</h3>\n\nWeb scraping involves retrieving data from web pages without a <b style='color: forestgreen;'>browser's</b> rendering aid. <b style='color: forestgreen;'>Get requests</b> fetch page content by sending a request via a <b style='color: forestgreen;'>network</b> to a server. You might think of this as a conversation between your computer and a <b style='color: forestgreen;'>server</b>, where data packets are exchanged:\n\n1. <b style='color: forestgreen;'>Client's machine</b> sends packets with a <b style='color: forestgreen;'>destination</b> to a server.\n2. <b style='color: forestgreen;'>Packets</b> traverse multiple networks until they reach the <b style='color: forestgreen;'>server</b>.\n3. <b style='color: forestgreen;'>Server</b> sends the requested page as packets back to the <b style='color: forestgreen;'>client's machine</b>. \n\nWhile <b style='color: forestgreen;'>browsers</b> make web <b style='color: forestgreen;'>navigation</b> seamless by interpreting code, these insights reveal the <b style='color: forestgreen;'>behind-the-scenes mechanics</b> essential for web scraping.\n\n<h3 style='color: forestgreen;'>Hands-On Web Scraping with Python</h3>\n\nPython offers simple tools for web scraping using libraries like <b style='color: forestgreen;'>BeautifulSoup</b>. Extract data by sending <b style='color: forestgreen;'>Get requests</b> and processing the <b style='color: forestgreen;'>HTML output</b>. Run the following Python snippet to get an HTML page:\n\n```python\nfrom urllib.request import urlopen\nhtml = urlopen('http://pythonscraping.com/pages/page1.html')\nprint(html.read())\n```\n\nThis outputs the <b style='color: forestgreen;'>HTML content</b>, acting as your starting point for deeper data extraction while bypassing the <b style='color: forestgreen;'>user-friendly interface</b> of browsers. Dive deeper with tools like <b style='color: forestgreen;'>BeautifulSoup</b> to parse and navigate HTML structures efficiently."
        },
        {
            "title": "Precise Selections & Regular Expressions with BeautifulSoup",
            "body": "When using <b style='color: forestgreen;'>BeautifulSoup</b>, it's important to be specific in your tag selections to ensure your <b style='color: forestgreen;'>web scraper</b> remains reliable even as page layouts change. Instead of selecting the first occurrence of a tag, make use of tag attributes available to uniquely identify elements, like using <b style='color: forestgreen;'>bs.find</b> with <b style='color: forestgreen;'>{'id':'giftList'}</b>. To gather multiple elements, remember functions like <b style='color: forestgreen;'>previous<em style='color: forestgreen;'>siblings</b> and <b style='color: forestgreen;'>next</em>sibling</b>, which locate adjacent elements, and <b style='color: forestgreen;'>parent</b> functions for locating parent tags. \n\nIf you're dealing with certain patterns like email addresses, <b style='color: forestgreen;'>regular expressions (RegEx)</b> can help significantly to define what's considered valid data (e.g., <b style='color: forestgreen;'>[A-Za-z0-9\\.<em style='color: forestgreen;'>+]+@[A-Za-z]+\\.(com|org|edu|net)</b> for emails). Practice with regular expressions, using a tool like <b style='color: forestgreen;'>Regex Pal</b>, makes it easier to handle large documents by filtering based on patterns. Using RegEx with BeautifulSoup allows flexible data extraction, <b style='color: forestgreen;'>bs.find</em>all('img', {'src':re.compile('pattern')})</b> gets specific items like URLs ending in a predefined format."
        },
        {
            "title": "Building a Resilient Web Crawler",
            "body": "When building a web scraper that visits *multiple websites* or *unknown sites* via crawling, developers must *plan cautiously*. Questions to ask include: *specific data needed*, *legal restrictions*, and *scraping strategy*. A Python *function set* under 60 lines can handle web crawling, but due care is important to enhance resilience against pitfalls, such as hitting Python\u2019s recursion limit if a site lacks external links. Always prepare *error handling functions* to tackle potential exceptions when operating a crawler at scale."
        },
        {
            "title": "Storing and Downloading Media Files",
            "body": "<b style='color: forestgreen;'>Storing Data from Web Scraping</b>\n\nWhen working with <b style='color: forestgreen;'>web scraping</b>, it\u2019s crucial to determine how the data will be stored. This decision impacts both the speed of your application and the load on the target server. While <b style='color: forestgreen;'>storing URLs</b> of media files (such as images) is <b style='color: forestgreen;'>efficient and less burdensome</b> on servers, direct <b style='color: forestgreen;'>file downloads</b> might be preferable if you anticipate frequent access or potential changes to the files on the server. Libraries like <b style='color: forestgreen;'>urllib</b> in Python can help download files directly by providing functions like <b style='color: forestgreen;'>urlretrieve</b> that store files locally. This method of downloading is <b style='color: forestgreen;'>simple and effective</b> for small-scale projects or needs."
        },
        {
            "title": "Downloading, CSV Handling, and File Cautions",
            "body": "<h3 style='color: forestgreen;'>Summary:</h3>\n1. <b style='color: forestgreen;'>Downloading Files Cautiously</b>\n   - Be careful downloading files. Running <b style='color: forestgreen;'>unknown scripts</b> or <b style='color: forestgreen;'>executables</b> can be risky, especially as an administrator. Always verify what you download, back up files, and use <b style='color: forestgreen;'>common sense</b>.\n\n2. <b style='color: forestgreen;'>CSV Files</b>\n   - CSV (Comma-Separated Values) is a simple and popular file format for data storage, which can be easily created and modified with Python's <b style='color: forestgreen;'>csv library</b>.\n\n3. <b style='color: forestgreen;'>Example CSV Creation</b>\n   - Open a CSV file, and use `<b style='color: forestgreen;'>csv.writer</b>` to add rows of <b style='color: forestgreen;'>data</b>. This process is flexible and straightforward.\n\n4. <b style='color: forestgreen;'>Creating CSV from HTML</b>\n   - Use <b style='color: forestgreen;'>BeautifulSoup</b> to parse an HTML table, clean content, and write it into a CSV using a few lines of code.\n\n5. <b style='color: forestgreen;'>Efficient Table Scraping</b>\n   - For one-time tasks, copying HTML tables directly into <b style='color: forestgreen;'>Excel</b> or <b style='color: forestgreen;'>Google Docs</b> is more efficient than coding a solution."
        },
        {
            "title": "Reading Documents and Understanding Encoding",
            "body": "<b style='color: forestgreen;'>Documents</b> on the internet are encoded in bits, defining how characters or pixels are seen. <b style='color: forestgreen;'>Text files</b>, including PDFs or Word docs, aren't as scary to manage with Python libraries, as explained in this chapter. Other than accessing <b style='color: forestgreen;'>HTML</b>, learning to read different file types opens a world of data riches. \n\nHandling these involves understanding <b style='color: forestgreen;'>document encoding</b>. Three major <b style='color: forestgreen;'>encoding standards</b> are ASCII, ISO, and Unicode. ASCII is English-centric with only 7 bits per character, initially holding 128 characters. Later, this was padded to 8 bits. Unicode, or UTF-8, accommodates a wider range, using 1 to 4 bytes per character, adapting to languages beyond English. ISO helps cover specific languages. These encoding standards often differ; about <b style='color: forestgreen;'>9%</b> of websites use some ISO encoding, so recognize and adjust for it before scraping."
        },
        {
            "title": "Cleaning Data with GREL in OpenRefine",
            "body": "OpenRefine offers several functions for cleaning and transforming data using its language, <b style='color: forestgreen;'>GREL</b>. For example, applying a function like `if(value.length() != 4, \"invalid\", value)` can help identify and manage invalid values like release dates that are not in the desired year format (<b style='color: forestgreen;'>YYYY</b>). This function marks the values that do not match the format as \"invalid\", helping data analysts easily spot errors.\n\nAnother approach involves using the `value.match` function with regular expressions. This function captures patterns like a <b style='color: forestgreen;'>four-digit year</b> from text, making it easier to standardize dates even from poorly formatted data. By using GREL in <b style='color: forestgreen;'>OpenRefine</b>, data can be more uniformly structured, improving its quality and reliability for further analysis or use."
        },
        {
            "title": "Simplifying HTTP Requests and API Access",
            "body": "Python's Requests library simplifies HTTP requests and form submissions without dealing with the complexities of older libraries like urllib2. For basic form submissions, you can post data directly to the action page specified in the form. When scraping websites with forms, ensure that parameter names match those expected by the server. <b style='color: forestgreen;'>JavaScript</b> is commonly used in web forms for dynamic content, but Python's Requests library can handle static form submissions easily.\n\nHandling more complex forms with non-text inputs or dealing with sessions and logins requires understanding the structure of forms and how cookies work. <b style='color: forestgreen;'>Selenium</b> can execute JavaScript and navigate as a user would in a browser, making it useful for pages that rely heavily on client-side scripting like Ajax. For APIs or sites using JavaScript for content dynamically, leveraging APIs directly can avoid the need to scrape HTML, especially when using <b style='color: forestgreen;'>RESTful</b> or <b style='color: forestgreen;'>GraphQL APIs</b> to access structured data directly."
        },
        {
            "title": "Introduction to OCR and Image Processing Libraries",
            "body": "We start with an overview of libraries that are crucial for image processing and text recognition, focusing on <b style='color: forestgreen;'>Pillow</b> for image manipulation and <b style='color: forestgreen;'>Tesseract</b> for OCR (Optical Character Recognition). Pillow, originally forked from PIL, is ideal for preprocessing images, like blurring with `ImageFilter.GaussianBlur`. Tesseract, though known for its <b style='color: forestgreen;'>accuracy and flexibility</b> in OCR tasks, can be enhanced through a Python wrapper, <b style='color: forestgreen;'>pytesseract</b>.\n\n<b style='color: forestgreen;'>Installing Tesseract</b> involves downloading executables or using package managers like <b style='color: forestgreen;'>apt-get</b> for Linux, <b style='color: forestgreen;'>Homebrew</b> for Mac, and an executable installer on Windows. The text outlines how to use pytesseract for recognizing text in images, such as using `pytesseract.image<em style='color: forestgreen;'>to</em>string()` to read and output text. Furthermore, it highlights some <b style='color: forestgreen;'>advanced capabilities</b> like returning bounding boxes and detailed data, illustrating the potential for applications beyond basic OCR tasks."
        },
        {
            "title": "Testing Web Frontends Using Scrapers",
            "body": "<b style='color: forestgreen;'>Web testing</b> often focuses on the backend, leaving the <b style='color: forestgreen;'>frontend</b> overlooked, although it's <b style='color: forestgreen;'>customer-facing</b>. While programming languages have <b style='color: forestgreen;'>test frameworks</b>, web <b style='color: forestgreen;'>frontends</b> lack consistent testing due to their complex mix of languages. This chapter emphasizes using <b style='color: forestgreen;'>web scrapers</b> for <b style='color: forestgreen;'>frontend tests</b> in <b style='color: forestgreen;'>test-driven development</b>. By replacing checklists with <b style='color: forestgreen;'>unit tests</b> and automating interface checks, you can streamline <b style='color: forestgreen;'>website functionality testing</b>, ensuring all elements function as designed across updates. <b style='color: forestgreen;'>Daily tests</b> enhance reliability, catching discrepancies from UI changes or code additions promptly."
        },
        {
            "title": "Running Web Scrapers on Remote Servers",
            "body": "<b style='color: forestgreen;'>Web Scraping on Remote Servers</b> \nRunning web scrapers on local machines is common, but shifting to remote platforms can offer several advantages like <b style='color: forestgreen;'>greater power</b>, <b style='color: forestgreen;'>flexibility</b>, and importantly, avoiding IP address blocks by using different IP addresses. IP addresses can\u2019t be faked \n<b style='color: forestgreen;'>Blocking challenges</b>: Although IP blocking is a last resort, it can affect legitimate users due to <b style='color: forestgreen;'>clustering</b> offenses. Hence, you need tools to run Python scrapers on remote servers, which are not very hard to set up and make life easier, especially when dealing with IP blocking issues.\n<b style='color: forestgreen;'>Reasons for Remote Scraping</b>: \n- <b style='color: forestgreen;'>Power & Flexibility</b>: Scripts aren\u2019t bound by personal computer limitations.\n- <b style='color: forestgreen;'>IP Address Usage</b>: Prevents IP blocking by using different server addresses."
        },
        {
            "title": "Understanding Tor for Scraping",
            "body": "<b style='color: forestgreen;'>Tor</b> is a network used for routing traffic through <b style='color: forestgreen;'>multiple servers</b> to <b style='color: forestgreen;'>hide origins</b> and <b style='color: forestgreen;'>obscure communications</b>. It is popular among <b style='color: forestgreen;'>human rights workers</b> and <b style='color: forestgreen;'>whistleblowers</b> for maintaining <b style='color: forestgreen;'>anonymity</b>. However, anonymity isn't perfect\u2014actions like <b style='color: forestgreen;'>logging in</b> can reveal your identity. For instance, a Harvard student was identified after sending a bomb threat using Tor because they were the only one logged into Tor at that time. <b style='color: forestgreen;'>Logging into Tor</b> doesn't mean you have <b style='color: forestgreen;'>complete invisibility</b>; use it cautiously and ethically."
        }
    ]
}